[
  {
    "objectID": "posts/Blog3/index.html",
    "href": "posts/Blog3/index.html",
    "title": "Linear Progression",
    "section": "",
    "text": "In this blog post, we will showcase a simple linear regression model using a real dataset named the “Auto Insurance” dataset. The objective is to illustrate the application of simple linear regression, specifically predicting the total payment for all claims in thousands of currency (y) based on the total number of claims (x) in the dataset.\n\nImporting Libraries\nIn this section, essential libraries for data manipulation, mathematical operations, and visualization are imported.\n\nimport pandas as pd\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nimport plotly.express as px\n\n\n\nExploring Dataset and Visualization\nIn this section, the “insurance.csv” file is loaded into a pandas DataFrame. The exploration of the data is initiated by printing information about the DataFrame, its columns, and the first 10 rows. Additionally, data visualization is performed through the creation of box plots for the ‘X’ and ‘Y’ columns, as well as a scatter plot depicting the relationship between ‘X’ and ‘Y’.\n\ndata = pd.read_csv('insurance.csv')\nprint(data.columns)\ndata.head(10)\n\nIndex(['X', 'Y'], dtype='object')\n\n\n\n\n\n\n\n\n\nX\nY\n\n\n\n\n0\n108\n392.5\n\n\n1\n19\n46.2\n\n\n2\n13\n15.7\n\n\n3\n124\n422.2\n\n\n4\n40\n119.4\n\n\n5\n57\n170.9\n\n\n6\n23\n56.9\n\n\n7\n14\n77.5\n\n\n8\n45\n214.0\n\n\n9\n10\n65.3\n\n\n\n\n\n\n\n\nfig = px.box(data['X'], points = 'all')\nfig.update_layout(title = f'Distribution of X',title_x=0.5, yaxis_title= \"Number of Insurance Claims\")\nfig.show()\n\n\n                                                \n\n\n\nfig = px.box(data['Y'], points = 'all')\nfig.update_layout(title = f'Distribution of Y',title_x=0.5, yaxis_title= \"Amount of Insurance Paid\")\nfig.show()\n\n\n                                                \n\n\n\nfig = px.scatter(x = data['X'], y=data['Y'])\nfig.update_layout(xaxis_title= \"Number of Claims\", yaxis_title=\"Payment in Claims\", height = 500, width = 700)\nfig.update_xaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.update_yaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.show()\n\n\n                                                \n\n\n\n\nCalculating Statistics, Covariance and Coefficient of Linear Progression\nIn this section, the mean and variance of the ‘X’ and ‘Y’ columns are computed and printed. Additionally, the covariance between ‘X’ and ‘Y’ is calculated. Subsequently, the coefficients for the linear regression model are computed.\n\nmean_x = np.mean(data['X'])\nmean_y = np.mean(data['Y'])\nvar_x = np.var(data['X'])\nvar_y = np.var(data['Y'])\nprint('x stats: mean= %.3f   variance= %.3f' % (mean_x, var_x))\nprint('y stats: mean= %.3f   variance= %.3f' % (mean_y, var_y))\n\nx stats: mean= 22.905   variance= 536.658\ny stats: mean= 98.187   variance= 7505.052\n\n\n\ndef covariance(x, y):\n    mean_x = np.mean(x)\n    mean_y = np.mean(y)\n    covar = 0.0\n    for i in range(len(x)):\n        covar += (x[i] - mean_x) * (y[i] - mean_y)\n    return covar/len(x)\ncovar_xy = covariance(data['X'], data['Y'])\nprint(f'Cov(X,Y): {covar_xy}')\n\nCov(X,Y): 1832.0543461829182\n\n\n\nb1 = covar_xy / var_x\nb0 = mean_y - b1 * mean_x\n\nprint(f'Coefficents:\\n b0: {b0}  b1: {b1} ')\n\nCoefficents:\n b0: 19.99448575911478  b1: 3.413823560066368 \n\n\n# Predicting and Visualizing the Results\nIn this segment, the ‘Y’ values are predicted using the calculated coefficients, and a plot is generated to visually compare the actual and predicted values. This code effectively executes a simple linear regression on the dataset, aiming to model the relationship between a single feature (independent variable ‘X’) and a real-valued target (dependent variable ‘Y’).\n\nx = data['X'].values.copy()\ny_hat = b0 + b1 * x\ny = data['Y'].values\nimport plotly.graph_objects as go\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(x=data['X'], y=data['Y'], name='train', mode='markers', marker_color='rgba(152, 0, 0, .8)'))\nfig.add_trace(go.Scatter(x=data['X'], y=y_hat, name='prediction', mode='lines+markers', marker_color='rgba(0, 152, 0, .8)'))\n\nfig.update_layout(title_x=0.5, xaxis_title= \"Number of Claims\", yaxis_title=\"Payment in Claims\")\nfig.update_xaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.update_yaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.show()\n\n\n                                                \n\n\n\n\nConclusion\nThe fundamental objective of linear regression is to establish a model representing the relationship between a single feature, denoted as the independent variable ‘X,’ and a real-valued target, referred to as the dependent variable ‘Y.’ The underlying assumption is that a linear relationship exists between ‘X’ and ‘Y,’ and the model endeavors to identify the optimal fit line that best captures this relationship among the data points. The aim is to create a linear equation that can predict ‘Y’ based on the values of ‘X’ with the least amount of error."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Blog on Machine Learning.",
    "section": "",
    "text": "Linear Progression\n\n\n\n\n\n\n\n\n\n\n\n\nNov 2, 2023\n\n\n\n\n\n\n  \n\n\n\n\nHeart Disease Classification and Prediction using KNN Model\n\n\n\n\n\n\n\n\n\n\n\n\nOct 13, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Electrical engineering professional with master’s degree from Virginia Tech. Interested in the semiconductor industry. Backed with 6 months of industrial and over 3 years of academic experience in project management, cross-functional teams, and leadership. Skilled in coordinating projects and working in a fast-paced environment."
  },
  {
    "objectID": "posts/Blog1/index.html",
    "href": "posts/Blog1/index.html",
    "title": "Heart Disease Classification and Prediction using KNN Model",
    "section": "",
    "text": "In this blog post, our focus will be on delving into a machine-learning project that revolves around predicting heart disease through KNN classification. The journey begins with a comprehensive understanding of the dataset, followed by visualizing key features and normalizing the data. The dataset encompasses information such as gender, age, blood pressure, sugar, and more. Subsequently, we will proceed to train and evaluate a KNN classification model to predict heart disease through machine learning.\n\nImporting Libraries\nThis segment imports essential libraries for data manipulation, visualization, and machine learning.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nimport os\n\n\n\nExploring Dataset and Visualization\nIn this segment, the dataset is explored first. Then it is examined through the calculation and display of the count and percentage of patients with and without heart disease, as well as the percentage of male and female patients. Additionally, it includes visualizations depicting the count of patients with and without heart disease, along with the count of male and female patients.\n\ndf = pd.read_csv(\"heart_Disease.csv\")\ndf.target.value_counts()\n\ntarget\n1    165\n0    138\nName: count, dtype: int64\n\n\n\nsns.countplot(x=\"target\", data=df, palette=\"bwr\")\nplt.show()\n\nC:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_17456\\2218517508.py:1: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n\n\n\n\n\n\n\ncountNoDisease = len(df[df.target == 0])\ncountHaveDisease = len(df[df.target == 1])\nprint(\"Percentage of Patients with no Heart Disease: {:.2f}%\".format((countNoDisease / (len(df.target))*100)))\nprint(\"Percentage of Patients with Heart Disease: {:.2f}%\".format((countHaveDisease / (len(df.target))*100)))\n\nPercentage of Patients with no Heart Disease: 45.54%\nPercentage of Patients with Heart Disease: 54.46%\n\n\n\ncountFemale = len(df[df.sex == 0])\ncountMale = len(df[df.sex == 1])\nprint(\"Percentage of Female Patients: {:.2f}%\".format((countFemale / (len(df.sex))*100)))\nprint(\"Percentage of Male Patients: {:.2f}%\".format((countMale / (len(df.sex))*100)))\n\nPercentage of Female Patients: 31.68%\nPercentage of Male Patients: 68.32%\n\n\n\nsns.countplot(x='sex', data=df, palette=\"mako_r\")\nplt.xlabel(\"Sex (0 = female, 1= male)\")\nplt.show()\n\nC:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_17456\\1879341537.py:1: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n\n\n\n\n\n\n\n\nNormalization of Data\nThis segment isolates the target variable from the predictors, normalizes the predictor data, divides the data into training and testing sets, and transposes the matrices to align with the specifications of the KNN model. We will partition our dataset, allocating 80% for training data and reserving 20% for testing purposes.\n\ny = df.target.values\nx_data = df.drop(['target'], axis = 1)\nx = (x_data - np.min(x_data)) / (np.max(x_data) - np.min(x_data))\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state=0)\nx_train = x_train.T\ny_train = y_train.T\nx_test = x_test.T\ny_test = y_test.T\n\nIn this portion, the code imports the KNeighborsClassifier from the sklearn library, initializes it with a parameter n_neighbors (k) set to 2, fits the model to the training data, generates predictions on the test data, and subsequently prints the accuracy of the model.\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 2)  # n_neighbors means k\nknn.fit(x_train.T, y_train.T)\nprediction = knn.predict(x_test.T)\nprint(\"{} NN Score: {:.2f}%\".format(2, knn.score(x_test.T, y_test.T)*100))\n\n2 NN Score: 60.66%\n\n\nThis segment executes a loop across values of k ranging from 1 to 20. For each iteration, it fits a KNN model, records the accuracy of each model in the “score list”, and subsequently plots the accuracies against the corresponding k values. The code identifies the maximum accuracy, stores it in “accuracies[‘KNN’]”, and prints this maximum accuracy. This process aids in pinpointing the optimal k value that yields the highest accuracy.\n\naccuracies = {}\nscoreList = []\nfor i in range(1,20):\n    knn2 = KNeighborsClassifier(n_neighbors = i)  # n_neighbors means k\n    knn2.fit(x_train.T, y_train.T)\n    scoreList.append(knn2.score(x_test.T, y_test.T))\n    \nplt.plot(range(1,20), scoreList)\nplt.xticks(np.arange(1,20,1))\nplt.xlabel(\"K value\")\nplt.ylabel(\"Score\")\nplt.show()\nacc = max(scoreList)*100\naccuracies['KNN'] = acc\nprint(\"Maximum KNN Score is {:.2f}%\".format(acc))\n\n\n\n\nMaximum KNN Score is 72.13%\n\n\n\n\nConclusion\nThe KNN model classification has been executed with an accuracy of 72.13%."
  }
]