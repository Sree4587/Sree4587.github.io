[
  {
    "objectID": "posts/Blog3/index.html",
    "href": "posts/Blog3/index.html",
    "title": "Linear Progression",
    "section": "",
    "text": "In this blog post, we will showcase a simple linear regression model using a real dataset named the “Auto Insurance” dataset. The objective is to illustrate the application of simple linear regression, specifically predicting the total payment for all claims in thousands of currency (y) based on the total number of claims (x) in the dataset.\n\nImporting Libraries\nIn this section, essential libraries for data manipulation, mathematical operations, and visualization are imported.\n\nimport pandas as pd\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nimport plotly.express as px\n\n\n\nExploring Dataset and Visualization\nIn this section, the “insurance.csv” file is loaded into a pandas DataFrame. The exploration of the data is initiated by printing information about the DataFrame, its columns, and the first 10 rows. Additionally, data visualization is performed through the creation of box plots for the ‘X’ and ‘Y’ columns, as well as a scatter plot depicting the relationship between ‘X’ and ‘Y’.\n\ndata = pd.read_csv('insurance.csv')\nprint(data.columns)\ndata.head(10)\n\nIndex(['X', 'Y'], dtype='object')\n\n\n\n\n\n\n\n\n\nX\nY\n\n\n\n\n0\n108\n392.5\n\n\n1\n19\n46.2\n\n\n2\n13\n15.7\n\n\n3\n124\n422.2\n\n\n4\n40\n119.4\n\n\n5\n57\n170.9\n\n\n6\n23\n56.9\n\n\n7\n14\n77.5\n\n\n8\n45\n214.0\n\n\n9\n10\n65.3\n\n\n\n\n\n\n\n\nfig = px.box(data['X'], points = 'all')\nfig.update_layout(title = f'Distribution of X',title_x=0.5, yaxis_title= \"Number of Insurance Claims\")\nfig.show()\n\n\n                                                \n\n\n\nfig = px.box(data['Y'], points = 'all')\nfig.update_layout(title = f'Distribution of Y',title_x=0.5, yaxis_title= \"Amount of Insurance Paid\")\nfig.show()\n\n\n                                                \n\n\n\nfig = px.scatter(x = data['X'], y=data['Y'])\nfig.update_layout(xaxis_title= \"Number of Claims\", yaxis_title=\"Payment in Claims\", height = 500, width = 700)\nfig.update_xaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.update_yaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.show()\n\n\n                                                \n\n\n\n\nCalculating Statistics, Covariance and Coefficient of Linear Progression\nIn this section, the mean and variance of the ‘X’ and ‘Y’ columns are computed and printed. Additionally, the covariance between ‘X’ and ‘Y’ is calculated. Subsequently, the coefficients for the linear regression model are computed.\n\nmean_x = np.mean(data['X'])\nmean_y = np.mean(data['Y'])\nvar_x = np.var(data['X'])\nvar_y = np.var(data['Y'])\nprint('x stats: mean= %.3f   variance= %.3f' % (mean_x, var_x))\nprint('y stats: mean= %.3f   variance= %.3f' % (mean_y, var_y))\n\nx stats: mean= 22.905   variance= 536.658\ny stats: mean= 98.187   variance= 7505.052\n\n\n\ndef covariance(x, y):\n    mean_x = np.mean(x)\n    mean_y = np.mean(y)\n    covar = 0.0\n    for i in range(len(x)):\n        covar += (x[i] - mean_x) * (y[i] - mean_y)\n    return covar/len(x)\ncovar_xy = covariance(data['X'], data['Y'])\nprint(f'Cov(X,Y): {covar_xy}')\n\nCov(X,Y): 1832.0543461829182\n\n\n\nb1 = covar_xy / var_x\nb0 = mean_y - b1 * mean_x\n\nprint(f'Coefficents:\\n b0: {b0}  b1: {b1} ')\n\nCoefficents:\n b0: 19.99448575911478  b1: 3.413823560066368 \n\n\n# Predicting and Visualizing the Results\nIn this segment, the ‘Y’ values are predicted using the calculated coefficients, and a plot is generated to visually compare the actual and predicted values. This code effectively executes a simple linear regression on the dataset, aiming to model the relationship between a single feature (independent variable ‘X’) and a real-valued target (dependent variable ‘Y’).\n\nx = data['X'].values.copy()\ny_hat = b0 + b1 * x\ny = data['Y'].values\nimport plotly.graph_objects as go\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(x=data['X'], y=data['Y'], name='train', mode='markers', marker_color='rgba(152, 0, 0, .8)'))\nfig.add_trace(go.Scatter(x=data['X'], y=y_hat, name='prediction', mode='lines+markers', marker_color='rgba(0, 152, 0, .8)'))\n\nfig.update_layout(title_x=0.5, xaxis_title= \"Number of Claims\", yaxis_title=\"Payment in Claims\")\nfig.update_xaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.update_yaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.show()\n\n\n                                                \n\n\n\n\nConclusion\nThe fundamental objective of linear regression is to establish a model representing the relationship between a single feature, denoted as the independent variable ‘X,’ and a real-valued target, referred to as the dependent variable ‘Y.’ The underlying assumption is that a linear relationship exists between ‘X’ and ‘Y,’ and the model endeavors to identify the optimal fit line that best captures this relationship among the data points. The aim is to create a linear equation that can predict ‘Y’ based on the values of ‘X’ with the least amount of error."
  },
  {
    "objectID": "posts/Blog1/index.html",
    "href": "posts/Blog1/index.html",
    "title": "Heart Disease Classification and Prediction using KNN Model",
    "section": "",
    "text": "In this blog post, our focus will be on delving into a machine-learning project that revolves around predicting heart disease through KNN classification. The journey begins with a comprehensive understanding of the dataset, followed by visualizing key features and normalizing the data. The dataset encompasses information such as gender, age, blood pressure, sugar, and more. Subsequently, we will proceed to train and evaluate a KNN classification model to predict heart disease through machine learning.\n\nImporting Libraries\nThis segment imports essential libraries for data manipulation, visualization, and machine learning.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nimport os\n\n\n\nExploring Dataset and Visualization\nIn this segment, the dataset is explored first. Then it is examined through the calculation and display of the count and percentage of patients with and without heart disease, as well as the percentage of male and female patients. Additionally, it includes visualizations depicting the count of patients with and without heart disease, along with the count of male and female patients.\n\ndf = pd.read_csv(\"heart_Disease.csv\")\ndf.target.value_counts()\n\ntarget\n1    165\n0    138\nName: count, dtype: int64\n\n\n\nsns.countplot(x=\"target\", data=df, palette=\"bwr\")\nplt.show()\n\nC:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_16460\\2218517508.py:1: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n\n\n\n\n\n\n\ncountNoDisease = len(df[df.target == 0])\ncountHaveDisease = len(df[df.target == 1])\nprint(\"Percentage of Patients with no Heart Disease: {:.2f}%\".format((countNoDisease / (len(df.target))*100)))\nprint(\"Percentage of Patients with Heart Disease: {:.2f}%\".format((countHaveDisease / (len(df.target))*100)))\n\nPercentage of Patients with no Heart Disease: 45.54%\nPercentage of Patients with Heart Disease: 54.46%\n\n\n\ncountFemale = len(df[df.sex == 0])\ncountMale = len(df[df.sex == 1])\nprint(\"Percentage of Female Patients: {:.2f}%\".format((countFemale / (len(df.sex))*100)))\nprint(\"Percentage of Male Patients: {:.2f}%\".format((countMale / (len(df.sex))*100)))\n\nPercentage of Female Patients: 31.68%\nPercentage of Male Patients: 68.32%\n\n\n\nsns.countplot(x='sex', data=df, palette=\"mako_r\")\nplt.xlabel(\"Sex (0 = female, 1= male)\")\nplt.show()\n\nC:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_16460\\1879341537.py:1: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n\n\n\n\n\n\n\n\nNormalization of Data\nThis segment isolates the target variable from the predictors, normalizes the predictor data, divides the data into training and testing sets, and transposes the matrices to align with the specifications of the KNN model. We will partition our dataset, allocating 80% for training data and reserving 20% for testing purposes.\n\ny = df.target.values\nx_data = df.drop(['target'], axis = 1)\nx = (x_data - np.min(x_data)) / (np.max(x_data) - np.min(x_data))\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state=0)\nx_train = x_train.T\ny_train = y_train.T\nx_test = x_test.T\ny_test = y_test.T\n\nIn this portion, the code imports the KNeighborsClassifier from the sklearn library, initializes it with a parameter n_neighbors (k) set to 2, fits the model to the training data, generates predictions on the test data, and subsequently prints the accuracy of the model.\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 2)  # n_neighbors means k\nknn.fit(x_train.T, y_train.T)\nprediction = knn.predict(x_test.T)\nprint(\"{} NN Score: {:.2f}%\".format(2, knn.score(x_test.T, y_test.T)*100))\n\n2 NN Score: 60.66%\n\n\nThis segment executes a loop across values of k ranging from 1 to 20. For each iteration, it fits a KNN model, records the accuracy of each model in the “score list”, and subsequently plots the accuracies against the corresponding k values. The code identifies the maximum accuracy, stores it in “accuracies[‘KNN’]”, and prints this maximum accuracy. This process aids in pinpointing the optimal k value that yields the highest accuracy.\n\naccuracies = {}\nscoreList = []\nfor i in range(1,20):\n    knn2 = KNeighborsClassifier(n_neighbors = i)  # n_neighbors means k\n    knn2.fit(x_train.T, y_train.T)\n    scoreList.append(knn2.score(x_test.T, y_test.T))\n    \nplt.plot(range(1,20), scoreList)\nplt.xticks(np.arange(1,20,1))\nplt.xlabel(\"K value\")\nplt.ylabel(\"Score\")\nplt.show()\nacc = max(scoreList)*100\naccuracies['KNN'] = acc\nprint(\"Maximum KNN Score is {:.2f}%\".format(acc))\n\n\n\n\nMaximum KNN Score is 72.13%\n\n\n\n\nConclusion\nThe KNN model classification has been executed with an accuracy of 72.13%."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Electrical engineering professional with master’s degree from Virginia Tech. Interested in the semiconductor industry. Backed with 6 months of industrial and over 3 years of academic experience in project management, cross-functional teams, and leadership. Skilled in coordinating projects and working in a fast-paced environment."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Blog on Machine Learning.",
    "section": "",
    "text": "Linear Progression\n\n\n\n\n\n\n\n\n\n\n\n\nNov 2, 2023\n\n\n\n\n\n\n  \n\n\n\n\nClustering of Shopping Data with K-means Clustering\n\n\n\n\n\n\n\n\n\n\n\n\nOct 21, 2023\n\n\n\n\n\n\n  \n\n\n\n\nHeart Disease Classification and Prediction using KNN Model\n\n\n\n\n\n\n\n\n\n\n\n\nOct 13, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Blog2/index.html",
    "href": "posts/Blog2/index.html",
    "title": "Clustering of Shopping Data with K-means Clustering",
    "section": "",
    "text": "In this blog post, our primary focus will be on exploring a machine-learning project centered on the creation of customer clusters through K-means clustering. The process starts with gaining a thorough understanding of the dataset, followed by visualizing key features and processing the data. The dataset includes information such as gender, age, annual income, and spending score. Moving forward, we will undertake the task of training and evaluating a K-means clustering model to generate clusters based on this dataset.\n\nImporting Libraries\nIn this section, essential libraries are imported for various tasks. NumPy and Pandas are utilized for data manipulation, while Matplotlib.pyplot and Seaborn are employed for data visualization. Additionally, the KMeans module from sklearn.cluster is imported for implementing the clustering algorithm.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\n\n\n\nExploring Dataset and Visualization\nIn this segment, the dataset is explored first. In this code snippet, the pd.read_csv() function is employed to load the dataset named ‘Mall_Customers.csv’ into a pandas DataFrame, designated as mall_data. Subsequently, the head() and info() functions are utilized to provide a rapid overview of the dataset, displaying the initial rows and essential information such as data types and missing values.\n\nmall_data = pd.read_csv(\"Mall_Customers.csv\")\nmall_data.head()\nmall_data.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 200 entries, 0 to 199\nData columns (total 5 columns):\n #   Column                  Non-Null Count  Dtype \n---  ------                  --------------  ----- \n 0   CustomerID              200 non-null    int64 \n 1   Gender                  200 non-null    object\n 2   Age                     200 non-null    int64 \n 3   Annual Income (k$)      200 non-null    int64 \n 4   Spending Score (1-100)  200 non-null    int64 \ndtypes: int64(4), object(1)\nmemory usage: 7.9+ KB\n\n\n\nmall_data.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 200 entries, 0 to 199\nData columns (total 5 columns):\n #   Column                  Non-Null Count  Dtype \n---  ------                  --------------  ----- \n 0   CustomerID              200 non-null    int64 \n 1   Gender                  200 non-null    object\n 2   Age                     200 non-null    int64 \n 3   Annual Income (k$)      200 non-null    int64 \n 4   Spending Score (1-100)  200 non-null    int64 \ndtypes: int64(4), object(1)\nmemory usage: 7.9+ KB\n\n\nIn this section, the correlation matrix of the dataset is computed using the corr() function, and the results are visualized through a heatmap generated with the Seaborn library.\n\n# Exclude non-numeric columns before computing correlation\nnumeric_columns = mall_data.select_dtypes(include=[np.number])\ncorr = numeric_columns.corr()\n\n# Visualization of correlation matrix\nplt.figure(figsize=(8, 8))\nsns.heatmap(corr, cbar=True, square=True, fmt='.1f', annot=True, cmap='Reds')\n\n&lt;Axes: &gt;\n\n\n\n\n\nIn this segment, multiple plots are generated to visually represent the data distribution and relationships among various features. This includes count plots for ‘Gender’ and ‘Age’, as well as a bar plot illustrating the relationship between ‘Annual Income’ and ‘Spending Score’.\n\nplt.figure(figsize=(10,10))\nsns.countplot(x=\"Gender\", data=mall_data)\n\n&lt;Axes: xlabel='Gender', ylabel='count'&gt;\n\n\n\n\n\n\nplt.figure(figsize=(16,10))\nsns.countplot(x=\"Age\", data=mall_data)\n\n&lt;Axes: xlabel='Age', ylabel='count'&gt;\n\n\n\n\n\n\nplt.figure(figsize=(20,8))\nsns.barplot(x='Annual Income (k$)',y='Spending Score (1-100)',data=mall_data)\n\n&lt;Axes: xlabel='Annual Income (k$)', ylabel='Spending Score (1-100)'&gt;\n\n\n\n\n\n\n\nClusters\nThis section prepares the data for clustering and determines the optimal number of clusters using the Elbow Method. The iloc function is used to select specific columns from the DataFrame to create a new variable X, which will be used for clustering. The code then fits the K-Means algorithm to the data for a range of cluster numbers (from 1 to 10) and calculates the Within-Cluster-Sum-of-Squares (WCSS). Then, a plot is created to visualize the Within-Cluster Sum of Squares (WCSS) against the number of clusters. This plot is instrumental in identifying the ‘elbow,’ aiding in the determination of the optimal number of clusters for the dataset.\n\nX = mall_data.iloc[:,[2,3,4]].values\nwcss = []\nfor i in range(1,11): # It will find wcss value for different number of clusters (for 1 cluster, for 2...until 10 clusters) and put it in our list\n    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=50)\n    kmeans.fit(X)\n    wcss.append(kmeans.inertia_)\nsns.set()\nplt.plot(range(1,11),wcss)\nplt.xlabel(\"Number of clusters\")\nplt.ylabel(\"WCSS value\")\nplt.show()\n\nC:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n\n\n\n\n\nIn this step, the K-Means algorithm is applied to the data using the optimal number of clusters determined in the previous step. The fit_predict() function is employed to calculate cluster centers and predict the cluster index for each sample in the dataset. And then the clusters are visualized.\n\nkmeans = KMeans(n_clusters = 5, init = 'k-means++',random_state = 0)\ny = kmeans.fit_predict(X)\nfig = plt.figure(figsize = (10,10))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X[y == 0,0],X[y == 0,1],X[y == 0,2], s = 40 , color = 'red', label = \"cluster 1\")\nax.scatter(X[y == 1,0],X[y == 1,1],X[y == 1,2], s = 40 , color = 'blue', label = \"cluster 2\")\nax.scatter(X[y == 2,0],X[y == 2,1],X[y == 2,2], s = 40 , color = 'green', label = \"cluster 3\")\nax.scatter(X[y == 3,0],X[y == 3,1],X[y == 3,2], s = 40 , color = 'yellow', label = \"cluster 4\")\nax.scatter(X[y == 4,0],X[y == 4,1],X[y == 4,2], s = 40 , color = 'purple', label = \"cluster 5\")\nax.set_xlabel('Age of a customer--&gt;')\nax.set_ylabel('Anual Income--&gt;')\nax.set_zlabel('Spending Score--&gt;')\nax.legend()\nplt.show()\n\nC:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n\n\n\n\n\n\n\nConclusion\nAs observed, clusters 3 and 5 exhibit higher spending scores.\nFor Cluster 3, comprising individuals aged less than 30 with very high annual incomes, their elevated spending scores align with expectations. To sustain and enhance this trend, offering these individuals improved incentives and exclusive deals could be an effective strategy.\nCluster 5 consists of individuals aged less than 30 with comparatively lower incomes. Despite their lower financial capacity, this group shows a preference for shopping. To further engage them, enticing offers such as discounts and additional freebies could be implemented to augment their spending within the establishment."
  }
]